{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client = pd.read_excel(\"data/clients.xlsx\")\n",
    "df_action = pd.read_excel(\"data/actions.xlsx\")\n",
    "df_transactions = pd.read_excel(\"data/transactions.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_client and df_transactions on `client_id`\n",
    "df_client_trans = pd.merge(df_client, df_transactions, on=\"client_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the clients with no transaction\n",
    "df_client_trans[\"has_transaction\"] = (\n",
    "    df_client_trans[\"transaction_id\"].notna().astype(int)\n",
    ")\n",
    "df_client_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_summary = df_client_trans.groupby(\"client_id\").agg(\n",
    "    {\n",
    "        # Client information: Taking the first occurrence of each (they should be the same for each client)\n",
    "        \"client_country\": \"first\",\n",
    "        \"client_nationality\": \"first\",\n",
    "        \"client_city\": \"first\",\n",
    "        \"client_premium_status\": \"first\",\n",
    "        \"client_is_phone_contactable\": \"first\",\n",
    "        \"client_is_email_contactable\": \"first\",\n",
    "        \"client_is_instant_messaging_contactable\": \"first\",\n",
    "        \"client_is_contactable\": \"first\",\n",
    "        # Aggregating transactions-related data\n",
    "        \"has_transaction\": \"sum\",  # Number of transactions (since has_transaction = 1 if transaction exists)\n",
    "        \"product_quantity\": \"sum\",  # Total quantity of products purchased\n",
    "        \"gross_amount_euro\": \"sum\",  # Total amount spent\n",
    "        # Transaction date list (need to list dates of all transactions)\n",
    "        \"transaction_date\": lambda x: list(x) if not x.isna().all() else [],\n",
    "        # Most common product category, subcategory, and style\n",
    "        \"product_category\": lambda x: (\n",
    "            x.mode()[0] if not x.mode().empty else None\n",
    "        ),  # Most common value\n",
    "        \"product_subcategory\": lambda x: x.mode()[0] if not x.mode().empty else None,\n",
    "        \"product_style\": lambda x: x.mode()[0] if not x.mode().empty else None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Renaming columns after aggregation\n",
    "df_client_summary = df_client_summary.rename(\n",
    "    columns={\n",
    "        \"client_country\": \"Country\",\n",
    "        \"client_nationality\": \"Nationality\",\n",
    "        \"client_city\": \"City\",\n",
    "        \"client_premium_status\": \"Premium Status\",\n",
    "        \"client_is_phone_contactable\": \"Is Phone Contactable\",\n",
    "        \"client_is_email_contactable\": \"Is Email Contactable\",\n",
    "        \"client_is_instant_messaging_contactable\": \"Is Message Contactable\",\n",
    "        \"client_is_contactable\": \"Is Contactable\",\n",
    "        \"has_transaction\": \"Transaction Count\",\n",
    "        \"product_quantity\": \"Total Product Quantity\",\n",
    "        \"gross_amount_euro\": \"Total Spending\",\n",
    "        \"transaction_date\": \"Transaction Dates\",\n",
    "        \"product_category\": \"Most Frequent Product Category\",\n",
    "        \"product_subcategory\": \"Most Frequent Product Subcategory\",\n",
    "        \"product_style\": \"Most Frequent Product Style\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Creating additional derived columns\n",
    "df_client_summary[\"gross_amount_per_product\"] = np.where(\n",
    "    df_client_summary[\"Total Product Quantity\"] == 0,\n",
    "    0,\n",
    "    df_client_summary[\"Total Spending\"] / df_client_summary[\"Total Product Quantity\"],\n",
    ")\n",
    "\n",
    "df_client_summary[\"gross_amount_per_transaction\"] = np.where(\n",
    "    df_client_summary[\"Transaction Count\"] == 0,\n",
    "    0,\n",
    "    df_client_summary[\"Total Spending\"] / df_client_summary[\"Transaction Count\"],\n",
    ")\n",
    "\n",
    "df_client_summary[\"products_per_transaction\"] = np.where(\n",
    "    df_client_summary[\"Transaction Count\"] == 0,\n",
    "    0,\n",
    "    df_client_summary[\"Total Product Quantity\"]\n",
    "    / df_client_summary[\"Transaction Count\"],\n",
    ")\n",
    "\n",
    "df_client_summary = df_client_summary.fillna(\"unknown\")\n",
    "\n",
    "# Reset index to make 'client_id' a column again\n",
    "df_client_summary = df_client_summary.reset_index()\n",
    "\n",
    "df_client_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clients that have a duplicate (same profile)\n",
    "df_without_id = df_client_summary.drop(columns=[\"client_id\", \"Transaction Dates\"])\n",
    "duplicate_rows = df_without_id.duplicated(keep=False)\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "print(f\"Number of rows that are duplicates lines: {num_duplicates}\")\n",
    "print(\n",
    "    f\"So, {num_duplicates/len(df_client_summary)*100}% of duplicates lines in the dataframe.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude `client_id` and unhashable columns\n",
    "df_without_id = df_client_summary.drop(columns=[\"client_id\", \"Transaction Dates\"])\n",
    "\n",
    "# Group by all columns and count occurrences\n",
    "duplicates_with_counts = (\n",
    "    df_without_id.groupby(df_without_id.columns.tolist())\n",
    "    .size()\n",
    "    .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "# Filter rows where duplicates occur (Count > 1)\n",
    "duplicates_only = duplicates_with_counts[duplicates_with_counts[\"Count\"] > 1]\n",
    "\n",
    "duplicates_only = duplicates_only.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "# Display the duplicates and their counts\n",
    "print(f\"Number of unique duplicate rows: {duplicates_only.shape[0]}\")\n",
    "duplicates_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some information about the event\n",
    "number_of_invitation = df_action.groupby(\"client_id\").agg(\n",
    "    {\"client_is_invited\": \"count\", \"action_start_date\": lambda x: list(x)}\n",
    ")\n",
    "\n",
    "number_of_invitation = number_of_invitation.rename(\n",
    "    columns={\n",
    "        \"client_is_invited\": \"Number of Invitation\",\n",
    "        \"action_start_date\": \"List of dates event\",\n",
    "    }\n",
    ")\n",
    "number_of_invitation = number_of_invitation.reset_index()\n",
    "\n",
    "df_client_summary = pd.merge(\n",
    "    df_client_summary, number_of_invitation, on=\"client_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "df_client_summary = df_client_summary.fillna(0)\n",
    "df_client_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check people that don't have any transaction but are invited\n",
    "people_without_trans_invited = df_client_summary[\n",
    "    (df_client_summary[\"Transaction Count\"] == 0)\n",
    "    & (df_client_summary[\"Number of Invitation\"] != 0)\n",
    "]\n",
    "print(people_without_trans_invited[\"Number of Invitation\"].unique())\n",
    "numerical_columns = [\n",
    "    \"Transaction Count\",\n",
    "    \"Total Product Quantity\",\n",
    "    \"Total Spending\",\n",
    "    \"gross_amount_per_product\",\n",
    "    \"gross_amount_per_transaction\",\n",
    "    \"products_per_transaction\",\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    \"Country\",\n",
    "    \"Premium Status\",\n",
    "    \"Nationality\",\n",
    "    \"City\",\n",
    "    \"Most Frequent Product Category\",\n",
    "    \"Most Frequent Product Subcategory\",\n",
    "    \"Most Frequent Product Style\",\n",
    "]\n",
    "\n",
    "\n",
    "# Check for None/NaN, 0, or [] across all rows\n",
    "def check_invalid_rows(row):\n",
    "    # Check if all numerical columns are 0 or NaN\n",
    "    numerical_check = all(\n",
    "        (row[col] == 0 or pd.isna(row[col])) for col in numerical_columns\n",
    "    )\n",
    "\n",
    "    # Check if any list-type columns (e.g., 'Transaction Dates') are empty\n",
    "    list_columns = [\"Transaction Dates\"]  # Add other list columns as needed\n",
    "    list_check = all(\n",
    "        (len(row[col]) == 0 if isinstance(row[col], list) else pd.isna(row[col]))\n",
    "        for col in list_columns\n",
    "    )\n",
    "\n",
    "    # Check if all categorical columns contain 'unknown'\n",
    "    categorical_check = all(\n",
    "        (row[col] == \"unknown\" or pd.isna(row[col])) for col in categorical_columns\n",
    "    )\n",
    "\n",
    "    # Check if all values are None, 0, or []\n",
    "    return numerical_check and list_check and categorical_check\n",
    "\n",
    "\n",
    "# Apply this check across all rows in the dataframe\n",
    "invalid_clients = people_without_trans_invited[\n",
    "    people_without_trans_invited.apply(check_invalid_rows, axis=1)\n",
    "]\n",
    "\n",
    "invalid_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the event information\n",
    "df_client_summary.drop(\n",
    "    columns=[\"Number of Invitation\", \"List of dates event\"], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the client with no information at all\n",
    "numerical_columns = [\n",
    "    \"Transaction Count\",\n",
    "    \"Total Product Quantity\",\n",
    "    \"Total Spending\",\n",
    "    \"gross_amount_per_product\",\n",
    "    \"gross_amount_per_transaction\",\n",
    "    \"products_per_transaction\",\n",
    "    \"Premium Status\",\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    \"Country\",\n",
    "    \"Nationality\",\n",
    "    \"City\",\n",
    "    \"Most Frequent Product Category\",\n",
    "    \"Most Frequent Product Subcategory\",\n",
    "    \"Most Frequent Product Style\",\n",
    "]\n",
    "\n",
    "\n",
    "# Check for None/NaN, 0, or [] across all rows\n",
    "def check_invalid_rows(row):\n",
    "    # Check if all numerical columns are 0 or NaN\n",
    "    numerical_check = all(\n",
    "        (row[col] == 0 or pd.isna(row[col])) for col in numerical_columns\n",
    "    )\n",
    "\n",
    "    list_columns = [\"Transaction Dates\"]\n",
    "    list_check = all(\n",
    "        (len(row[col]) == 0 if isinstance(row[col], list) else pd.isna(row[col]))\n",
    "        for col in list_columns\n",
    "    )\n",
    "\n",
    "    # Check if all categorical columns contain 'unknown'\n",
    "    categorical_check = all(\n",
    "        (row[col] == \"unknown\" or pd.isna(row[col])) for col in categorical_columns\n",
    "    )\n",
    "\n",
    "    # Check if all values are None, 0, or []\n",
    "    return numerical_check and list_check and categorical_check\n",
    "\n",
    "\n",
    "# Identify rows that meet the invalid condition\n",
    "invalid_rows_mask = df_client_summary.apply(check_invalid_rows, axis=1)\n",
    "\n",
    "# Remove invalid rows\n",
    "df_cleaned = df_client_summary[~invalid_rows_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original dataset size: {df_client_summary.shape[0]}\")\n",
    "print(f\"Cleaned dataset size: {df_cleaned.shape[0]}\")\n",
    "print(f\"Number of removed rows: {df_client_summary.shape[0] - df_cleaned.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a first clustering to found 5 categories of clients\n",
    "df_clustering = df_cleaned.copy()\n",
    "\n",
    "numerical_cols = [\n",
    "    \"Transaction Count\",\n",
    "    \"Total Product Quantity\",\n",
    "    \"Total Spending\",\n",
    "    \"gross_amount_per_product\",\n",
    "    \"gross_amount_per_transaction\",\n",
    "    \"products_per_transaction\",\n",
    "]\n",
    "categorical_cols = [\n",
    "    \"Country\",\n",
    "    \"Nationality\",\n",
    "    \"City\",\n",
    "    \"Most Frequent Product Category\",\n",
    "    \"Most Frequent Product Subcategory\",\n",
    "    \"Most Frequent Product Style\",\n",
    "]\n",
    "\n",
    "other_columns = [\n",
    "    \"Is Phone Contactable\",\n",
    "    \"Is Email Contactable\",\n",
    "    \"Is Message Contactable\",\n",
    "    \"Is Contactable\",\n",
    "]\n",
    "\n",
    "# Preprocessing: One-hot encoding categorical features and scaling numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"passthrough\", \"passthrough\", other_columns),\n",
    "    ]\n",
    ")\n",
    "clustering_data = preprocessor.fit_transform(df_clustering)\n",
    "if hasattr(clustering_data, \"toarray\"):  # Convert sparse matrix to dense format\n",
    "    clustering_data = clustering_data.toarray()\n",
    "\n",
    "\n",
    "# Perform weighted KMeans clustering\n",
    "mini_kmeans = AgglomerativeClustering(n_clusters=5)\n",
    "df_clustering[\"Category\"] = mini_kmeans.fit_predict(clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform subcategory clustering within each main categroy\n",
    "for cluster in df_clustering[\"Category\"].unique():\n",
    "    cluster_data = df_clustering[df_clustering[\"Category\"] == cluster]\n",
    "\n",
    "    # Preprocess the cluster-specific data\n",
    "    cluster_data_transformed = preprocessor.transform(\n",
    "        cluster_data.drop(columns=[\"Category\"])\n",
    "    )\n",
    "    if hasattr(cluster_data_transformed, \"toarray\"):\n",
    "        cluster_data_transformed = cluster_data_transformed.toarray()\n",
    "\n",
    "    # Check if there are enough points to create subcategories\n",
    "    if cluster_data.shape[0] > 3:  # Only cluster if enough data points exist\n",
    "        agg_sub = AgglomerativeClustering(n_clusters=3)\n",
    "        df_clustering.loc[df_clustering[\"Category\"] == cluster, \"Sub-Category\"] = (\n",
    "            agg_sub.fit_predict(cluster_data_transformed)\n",
    "        )\n",
    "    else:\n",
    "        df_clustering.loc[df_clustering[\"Category\"] == cluster, \"Sub-Category\"] = (\n",
    "            0  # Default subcategory if not enough points\n",
    "        )\n",
    "\n",
    "print(\"Subcategory clustering is complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the categories\n",
    "df_clustering[\"Category\"].value_counts().plot(\n",
    "    kind=\"bar\", title=\"Categories Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of clients in each category\n",
    "df_clustering[[\"Category\", \"Sub-Category\"]].value_counts().plot(\n",
    "    kind=\"bar\", title=\"Category-Subcategory Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Clusters using PCA\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(clustering_data)\n",
    "\n",
    "# Plot clusters\n",
    "plt.scatter(\n",
    "    reduced_features[:, 0],\n",
    "    reduced_features[:, 1],\n",
    "    c=df_clustering[\"Category\"],\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.title(\"Cluster Visualization (PCA)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any client appears multiple times for the same event\n",
    "duplicate_check = df_action.duplicated(\n",
    "    subset=[\"client_id\", \"action_type_label\", \"action_start_date\"], keep=False\n",
    ")\n",
    "\n",
    "# Display problematic duplicates\n",
    "print(f\"Number of duplicated client-event rows: {duplicate_check.sum()}\")\n",
    "df_action[duplicate_check].sort_values(by=[\"client_id\", \"action_type_label\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "df_action_unique = df_action.drop_duplicates(\n",
    "    subset=[\"client_id\", \"action_type_label\", \"action_start_date\"]\n",
    ")\n",
    "\n",
    "# Verify row count after deduplication\n",
    "print(f\"Original df_action size: {df_action.shape[0]}\")\n",
    "print(f\"After deduplication: {df_action_unique.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure event data is unique per event type, not per invite action\n",
    "df_events_unique = df_action_unique[\n",
    "    [\n",
    "        \"action_type_label\",\n",
    "        \"action_subcategory_label\",\n",
    "        \"action_start_date\",\n",
    "        \"action_year\",\n",
    "        \"action_end_date\",\n",
    "        \"action_collection_year\",\n",
    "        \"action_collection\",\n",
    "        \"action_universe\",\n",
    "        \"action_category_label\",\n",
    "        \"action_channel\",\n",
    "        \"action_label\",\n",
    "    ]\n",
    "].drop_duplicates()\n",
    "\n",
    "# Create Cartesian product: every client x every event\n",
    "df_full_events = df_clustering[[\"client_id\"]].merge(df_events_unique, how=\"cross\")\n",
    "\n",
    "# Set default values for 'client_is_invited' and 'client_is_present' to 0\n",
    "df_full_events[\"client_is_invited\"] = 0\n",
    "df_full_events[\"client_is_present\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with df_action to update invited and present clients\n",
    "df_full_events = df_full_events.merge(\n",
    "    df_action_unique[\n",
    "        [\n",
    "            \"client_id\",\n",
    "            \"action_type_label\",\n",
    "            \"action_subcategory_label\",\n",
    "            \"action_start_date\",\n",
    "            \"client_is_invited\",\n",
    "            \"client_is_present\",\n",
    "        ]\n",
    "    ],\n",
    "    on=[\n",
    "        \"client_id\",\n",
    "        \"action_type_label\",\n",
    "        \"action_subcategory_label\",\n",
    "        \"action_start_date\",\n",
    "    ],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_from_action\"),\n",
    ")\n",
    "\n",
    "# Resolve conflicts by prioritizing real invitation data\n",
    "df_full_events[\"client_is_invited\"] = df_full_events[\n",
    "    \"client_is_invited_from_action\"\n",
    "].fillna(df_full_events[\"client_is_invited\"])\n",
    "df_full_events[\"client_is_present\"] = df_full_events[\n",
    "    \"client_is_present_from_action\"\n",
    "].fillna(df_full_events[\"client_is_present\"])\n",
    "\n",
    "# Drop the extra columns from df_action merge\n",
    "df_full_events.drop(\n",
    "    columns=[\"client_is_invited_from_action\", \"client_is_present_from_action\"],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Ensure missing values are filled with 0 (for clients who were never invited)\n",
    "df_full_events[\"client_is_invited\"].fillna(0, inplace=True)\n",
    "df_full_events[\"client_is_present\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dates are in datetime format (only if not already)\n",
    "df_transactions[\"transaction_date\"] = pd.to_datetime(\n",
    "    df_transactions[\"transaction_date\"]\n",
    ")\n",
    "df_full_events[\"action_start_date\"] = pd.to_datetime(\n",
    "    df_full_events[\"action_start_date\"]\n",
    ")\n",
    "\n",
    "# Merge transactions with events based on client_id\n",
    "df_merged = df_transactions.merge(\n",
    "    df_full_events[[\"client_id\", \"action_start_date\"]], on=\"client_id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Keep only transactions that happened BEFORE the event date\n",
    "df_merged = df_merged[df_merged[\"transaction_date\"] < df_merged[\"action_start_date\"]]\n",
    "\n",
    "# Aggregate transaction data\n",
    "df_transaction_summary = (\n",
    "    df_merged.groupby([\"client_id\", \"action_start_date\"])\n",
    "    .agg(\n",
    "        total_transactions=(\"transaction_id\", \"count\"),\n",
    "        total_spending=(\"gross_amount_euro\", \"sum\"),\n",
    "        total_product_quantity=(\"product_quantity\", \"sum\"),\n",
    "        last_transaction_date=(\"transaction_date\", \"max\"),\n",
    "        most_frequent_product_category=(\n",
    "            \"product_category\",\n",
    "            lambda x: x.mode()[0] if not x.mode().empty else \"unknown\",\n",
    "        ),\n",
    "        most_frequent_product_subcategory=(\n",
    "            \"product_subcategory\",\n",
    "            lambda x: x.mode()[0] if not x.mode().empty else \"unknown\",\n",
    "        ),\n",
    "        most_frequent_product_style=(\n",
    "            \"product_style\",\n",
    "            lambda x: x.mode()[0] if not x.mode().empty else \"unknown\",\n",
    "        ),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "#  Merge back with df_full_events\n",
    "df_full_events = df_full_events.merge(\n",
    "    df_transaction_summary, on=[\"client_id\", \"action_start_date\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill missing values for clients with no transactions before the event\n",
    "df_full_events.fillna(\n",
    "    {\n",
    "        \"total_transactions\": 0,\n",
    "        \"total_spending\": 0,\n",
    "        \"total_product_quantity\": 0,\n",
    "        \"last_transaction_date\": pd.NaT,\n",
    "        \"most_frequent_product_category\": \"unknown\",\n",
    "        \"most_frequent_product_subcategory\": \"unknown\",\n",
    "        \"most_frequent_product_style\": \"unknown\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Add the clients information\n",
    "df_client_category = df_clustering.drop(\n",
    "    columns=[\n",
    "        \"Is Contactable\",\n",
    "        \"Transaction Count\",\n",
    "        \"Total Spending\",\n",
    "        \"Total Product Quantity\",\n",
    "        \"Transaction Dates\",\n",
    "        \"Most Frequent Product Category\",\n",
    "        \"Most Frequent Product Subcategory\",\n",
    "        \"Most Frequent Product Style\",\n",
    "        \"gross_amount_per_product\",\n",
    "        \"gross_amount_per_transaction\",\n",
    "        \"products_per_transaction\",\n",
    "    ]\n",
    ")\n",
    "df_final = df_full_events.merge(df_client_category, on=\"client_id\", how=\"left\")\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new column order\n",
    "new_column_order = [\n",
    "    # Client Information\n",
    "    \"client_id\",\n",
    "    \"Country\",\n",
    "    \"Nationality\",\n",
    "    \"City\",\n",
    "    \"Premium Status\",\n",
    "    \"Is Phone Contactable\",\n",
    "    \"Is Email Contactable\",\n",
    "    \"Is Message Contactable\",\n",
    "    \"Category\",\n",
    "    \"Sub-Category\",\n",
    "    # Transaction History (Before the Event)\n",
    "    \"total_transactions\",\n",
    "    \"total_spending\",\n",
    "    \"total_product_quantity\",\n",
    "    \"last_transaction_date\",\n",
    "    \"most_frequent_product_category\",\n",
    "    \"most_frequent_product_subcategory\",\n",
    "    \"most_frequent_product_style\",\n",
    "    # Event Information\n",
    "    \"action_type_label\",\n",
    "    \"action_subcategory_label\",\n",
    "    \"action_start_date\",\n",
    "    \"action_end_date\",\n",
    "    \"action_year\",\n",
    "    \"action_collection_year\",\n",
    "    \"action_collection\",\n",
    "    \"action_universe\",\n",
    "    \"action_category_label\",\n",
    "    \"action_channel\",\n",
    "    \"action_label\",\n",
    "    \"client_is_present\",\n",
    "    \"client_is_invited\",\n",
    "]\n",
    "\n",
    "# Rename columns for clarity\n",
    "column_renaming = {\n",
    "    # Transaction History\n",
    "    \"total_transactions\": \"Total Past Transactions\",\n",
    "    \"total_spending\": \"Total Past Spending (â‚¬)\",\n",
    "    \"total_product_quantity\": \"Total Past Products Purchased\",\n",
    "    \"last_transaction_date\": \"Last Purchase Date\",\n",
    "    \"most_frequent_product_category\": \"Most Purchased Product Category\",\n",
    "    \"most_frequent_product_subcategory\": \"Most Purchased Product Subcategory\",\n",
    "    \"most_frequent_product_style\": \"Most Purchased Product Style\",\n",
    "}\n",
    "\n",
    "# Reorder and rename columns\n",
    "df_final = df_final[new_column_order].rename(columns=column_renaming)\n",
    "\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the final dataframe\n",
    "df_final.to_csv(\"event_client.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
